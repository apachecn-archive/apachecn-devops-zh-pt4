# 9.在 SQL 数据库中捕获管道错误日志

在第 8 章[T4 中，我讨论了各种捕获 Azure 数据工厂管道日志并将数据持久化到 SQL 表或 Azure 数据湖存储 Gen2 中的方法。尽管当管道活动成功时，捕获管道日志数据的过程是有价值的，但本章将介绍如何捕获 Azure 数据工厂管道错误并将其持久化到前面章节中创建的 ADF_DB 内的 SQL 表中。此外，我们将回顾我在前面章节中讨论过的管道参数流程，以展示`pipeline_errors`、`pipeline_log`和`pipeline_parameter`表之间的相互关系。](08.html)

概括地说，这个过程需要的表格包括

*   管道参数

*   管道 _ 日志

*   管道 _ 错误

图 [9-1](#Fig1) 展示了这些表格是如何相互连接的。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig1_HTML.png](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig1_HTML.png)

图 9-1

描述管道错误、日志和参数表之间关系的图表

## 创建参数表

概括地说，我们在前几章中创建了一些`pipeline_parameter`表的变体。下面的脚本将创建一个更新版本的`pipeline_parameter`表，其中包含一些额外的列，并将列`parameter_id`作为主键。回想一下前面章节的练习，该表推动了元数据 ETL 方法:

```
SET ANSI_NULLS ON
GO

SET QUOTED_IDENTIFIER ON
go

CREATE TABLE [dbo].[pipeline_parameter]
  (
     [parameter_id]                       [INT] IDENTITY(1, 1) NOT NULL,
     [server_name]                        [NVARCHAR](500) NULL,
     [src_type]                           [NVARCHAR](500) NULL,
     [src_schema]                         [NVARCHAR](500) NULL,
     [src_db]                             [NVARCHAR](500) NULL,
     [src_name]                           [NVARCHAR](500) NULL,
     [dst_type]                           [NVARCHAR](500) NULL,
     [dst_name]                           [NVARCHAR](500) NULL,
     [include_pipeline_flag]              [NVARCHAR](500) NULL,
     [partition_field]                    [NVARCHAR](500) NULL,
     [process_type]                       [NVARCHAR](500) NULL,
     [priority_lane]                      [NVARCHAR](500) NULL,
     [pipeline_date]                      [NVARCHAR](500) NULL,
     [pipeline_status]                    [NVARCHAR](500) NULL,
     [load_synapse]                       [NVARCHAR](500) NULL,
     [load_frequency]                     [NVARCHAR](500) NULL,
     [dst_folder]                         [NVARCHAR](500) NULL,
     [file_type]                          [NVARCHAR](500) NULL,
     [lake_dst_folder]                    [NVARCHAR](500) NULL,
     [spark_flag]                         [NVARCHAR](500) NULL,
     [dst_schema]                         [NVARCHAR](500) NULL,
     [distribution_type]                  [NVARCHAR](500) NULL,
     [load_sqldw_etl_pipeline_date]       [DATETIME] NULL,
     [load_sqldw_etl_pipeline_status]     [NVARCHAR](500) NULL,
     [load_sqldw_curated_pipeline_date]   [DATETIME] NULL,
     [load_sqldw_curated_pipeline_status] [NVARCHAR](500) NULL,
     [load_delta_pipeline_date]           [DATETIME] NULL,
     [load_delta_pipeline_status]         [NVARCHAR](500) NULL,
     PRIMARY KEY CLUSTERED ( [parameter_id] ASC )WITH (statistics_norecompute =
     OFF, ignore_dup_key = OFF) ON [PRIMARY]
  )
ON [PRIMARY]

go

```

## 创建日志表

下一个脚本将创建用于捕获数据工厂成功日志的`pipeline_log`表。在该表中，`log_id`列是主键，`parameter_id`列是外键，引用参数表中的`parameter_id`列:

```
SET ansi_nulls ON

go

SET quoted_identifier ON

go

CREATE TABLE [dbo].[pipeline_log]
  (
     [log_id]                                [INT] IDENTITY(1, 1) NOT NULL,
     [parameter_id]                          [INT] NULL,
     [datafactory_name]                      [NVARCHAR](500) NULL,
     [pipeline_name]                         [NVARCHAR](500) NULL,
     [runid]                                 [NVARCHAR](500) NULL,
     [source]                                [NVARCHAR](500) NULL,
     [destination]                           [NVARCHAR](500) NULL,
     [triggertype]                           [NVARCHAR](500) NULL,
     [triggerid]                             [NVARCHAR](500) NULL,
     [triggername]                           [NVARCHAR](500) NULL,
     [triggertime]                           [NVARCHAR](500) NULL,
     [rowscopied]                            [NVARCHAR](500) NULL,
     [dataread]                              [INT] NULL,
     [no_parallelcopies]                     [INT] NULL,
     [copyduration_in_secs]                  [NVARCHAR](500) NULL,
     [effectiveintegrationruntime]           [NVARCHAR](500) NULL,
     [source_type]                           [NVARCHAR](500) NULL,
     [sink_type]                             [NVARCHAR](500) NULL,
     [execution_status]                      [NVARCHAR](500) NULL,
     [copyactivity_start_time]               [NVARCHAR](500) NULL,
     [copyactivity_end_time]                 [NVARCHAR](500) NULL,
     [copyactivity_queuingduration_in_secs]  [NVARCHAR](500) NULL,
     [copyactivity_transferduration_in_secs] [NVARCHAR](500) NULL,
     CONSTRAINT [PK_pipeline_log] PRIMARY KEY CLUSTERED ( [log_id] ASC )WITH (
     statistics_norecompute = OFF, ignore_dup_key = OFF) ON [PRIMARY]
  )
ON [PRIMARY]

go

ALTER TABLE [dbo].[pipeline_log]
  WITH CHECK ADD FOREIGN KEY([parameter_id]) REFERENCES
  [dbo].[pipeline_parameter] ([parameter_id]) ON UPDATE CASCADE

go 

```

## 创建一个错误表

创建一个错误表需要执行下一个脚本，该脚本将创建一个`pipeline_errors`表，用于从失败的管道活动中捕获数据工厂错误细节。在该表中，列`error_id`是主键，列`parameter_id`是外键，引用来自`pipeline_parameter`表的列`parameter_id`:

```
SET ansi_nulls ON

go

SET quoted_identifier ON

go

CREATE TABLE [dbo].[pipeline_errors]
  (
     [error_id]                    [INT] IDENTITY(1, 1) NOT NULL,
     [parameter_id]                [INT] NULL,
     [datafactory_name]            [NVARCHAR](500) NULL,
     [pipeline_name]               [NVARCHAR](500) NULL,
     [runid]                       [NVARCHAR](500) NULL,
     [source]                      [NVARCHAR](500) NULL,
     [destination]                 [NVARCHAR](500) NULL,
     [triggertype]                 [NVARCHAR](500) NULL,
     [triggerid]                   [NVARCHAR](500) NULL,
     [triggername]                 [NVARCHAR](500) NULL,
     [triggertime]                 [NVARCHAR](500) NULL,
     [no_parallelcopies]           [INT] NULL,
     [copyduration_in_secs]        [NVARCHAR](500) NULL,
     [effectiveintegrationruntime] [NVARCHAR](500) NULL,
     [source_type]                 [NVARCHAR](500) NULL,
     [sink_type]                   [NVARCHAR](500) NULL,
     [execution_status]            [NVARCHAR](500) NULL,
     [errordescription]            [NVARCHAR](max) NULL,
     [errorcode]                   [NVARCHAR](500) NULL,
     [errorloggedtime]             [NVARCHAR](500) NULL,
     [failuretype]                 [NVARCHAR](500) NULL,
     CONSTRAINT [PK_pipeline_error] PRIMARY KEY CLUSTERED ( [error_id] ASC )WITH
     (statistics_norecompute = OFF, ignore_dup_key = OFF) ON [PRIMARY]
  )
ON [PRIMARY]
textimage_on [PRIMARY]

go

ALTER TABLE [dbo].[pipeline_errors]
  WITH CHECK ADD FOREIGN KEY([parameter_id]) REFERENCES
  [dbo].[pipeline_parameter] ([parameter_id]) ON UPDATE CASCADE

go      

```

## 创建一个存储过程来更新日志表

现在您已经准备好了所有必要的 SQL 表，通过使用下面的脚本开始创建一些必要的存储过程，这将创建一个存储过程来用成功的管道运行中的数据更新`pipeline_log`表。请注意，此存储过程将在运行时从数据工厂管道中调用:

```
SET ansi_nulls ON

go

SET quoted_identifier ON

go

CREATE PROCEDURE [dbo].[usp_updatelogtable] @datafactory_name
VARCHAR(250),
                                           @pipeline_name
VARCHAR(250),
                                           @runid
VARCHAR(250),
                                           @source
VARCHAR(300),
                                           @destination
VARCHAR(300),
                                           @triggertype
VARCHAR(300),
                                           @triggerid
VARCHAR(300),
                                           @triggername
VARCHAR(300),
                                           @triggertime
VARCHAR(500),
                                           @rowscopied
VARCHAR(300),
                                           @dataread
INT,
                                           @no_parallelcopies
INT,
                                           @copyduration_in_secs
VARCHAR(300),
                                           @effectiveintegrationruntime
VARCHAR(300),
                                           @source_type
VARCHAR(300),
                                           @sink_type
VARCHAR(300),
                                           @execution_status
VARCHAR(300),
                                           @copyactivity_start_time
VARCHAR(500),
                                           @copyactivity_end_time
VARCHAR(500),
                                           @copyactivity_queuingduration_in_secs
VARCHAR(500),
@copyactivity_transferduration_in_secs VARCHAR(500)
AS
INSERT INTO [pipeline_log]

([datafactory_name],
[pipeline_name],
[runid],
[source],
[destination],
[triggertype],
[triggerid],
[triggername],
[triggertime],
[rowscopied],
[dataread],
[no_parallelcopies],
[copyduration_in_secs],
[effectiveintegrationruntime],
[source_type],
[sink_type],
[execution_status],
[copyactivity_start_time],
[copyactivity_end_time],
[copyactivity_queuingduration_in_secs],
[copyactivity_transferduration_in_secs])
VALUES      ( @datafactory_name,
@pipeline_name,
@runid,
@source,
@destination,
@triggertype,
@triggerid,
@triggername,
@triggertime,
@rowscopied,
@dataread,
@no_parallelcopies,
@copyduration_in_secs,
@effectiveintegrationruntime,
@source_type,
@sink_type,
@execution_status,
@copyactivity_start_time,
@copyactivity_end_time,
@copyactivity_queuingduration_in_secs,
@copyactivity_transferduration_in_secs )

go 

```

## 创建一个存储过程来更新错误表

接下来，运行下面的脚本，它将创建一个存储过程，用失败的管道运行中的详细错误数据更新`pipeline_errors`表。请注意，此存储过程将在运行时从数据工厂管道中调用:

```
SET ANSI_NULLS ON
GO

SET QUOTED_IDENTIFIER ON
GO

CREATE PROCEDURE [dbo].[usp_updateerrortable]
      @datafactory_name [nvarchar](500) NULL,
      @pipeline_name [nvarchar](500) NULL,
      @runid [nvarchar](500) NULL,
      @source [nvarchar](500) NULL,
      @destination [nvarchar](500) NULL,
      @triggertype [nvarchar](500) NULL,
      @triggerid [nvarchar](500) NULL,
      @triggername [nvarchar](500) NULL,
      @triggertime [nvarchar](500) NULL,
      @no_parallelcopies [int] NULL,
      @copyduration_in_secs [nvarchar](500) NULL,
      @effectiveintegrationruntime [nvarchar](500) NULL,
      @source_type [nvarchar](500) NULL,
      @sink_type [nvarchar](500) NULL,
      @execution_status [nvarchar](500) NULL,
      @errordescription [nvarchar](max) NULL,
      @errorcode [nvarchar](500) NULL,
      @errorloggedtime [nvarchar](500) NULL,
      @failuretype [nvarchar](500) NULL
AS
INSERT INTO [pipeline_errors]

(
    [datafactory_name],
      [pipeline_name],
      [runid],
      [source],
      [destination],
      [triggertype],
      [triggerid],
      [triggername],
      [triggertime],
      [no_parallelcopies],
      [copyduration_in_secs],
      [effectiveintegrationruntime],
      [source_type],
      [sink_type],
      [execution_status],
      [errordescription],
      [errorcode],
      [errorloggedtime],
      [failuretype]
)
VALUES
(
      @datafactory_name,
      @pipeline_name,
      @runid,
      @source,
      @destination,
      @triggertype,
      @triggerid,
      @triggername,
      @triggertime,
      @no_parallelcopies,
      @copyduration_in_secs,
      @effectiveintegrationruntime,
      @source_type,
      @sink_type,
      @execution_status,
      @errordescription,
      @errorcode,
      @errorloggedtime,
      @failuretype
)

GO

```

## 创建源错误

在第 [4](04.html) 章中，您启动了构建 ADF 管道的过程，将源 SQL Server 表加载到 Data Lake Storage Gen2。基于这个过程，让我们测试数据工厂管道中的一个已知错误，并为了这个练习的目的通过重新创建一个错误进行处理。通常，包含至少 8，000 个以上字符的 varchar(max)数据类型在加载到 Synapse Analytics 专用 SQL 池时会失败，因为 varchar(max)是不支持的数据类型。这似乎是一个很好的错误测试用例。

首先创建一个表(如`SalesLT.Address`)，该表将存储一大块文本，这些文本在加载到 Synapse Analytics 专用 SQL 池时最终会导致错误。图 [9-2](#Fig2) 所示的`SalesLT.Address`包含具有 varchar(max)数据类型的描述列。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig2_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig2_HTML.jpg)

图 9-2

SSMS 的销售观。地址

在`SalesLT.Address`内，添加一大块文本。例如，您可以输入一些大于 8001 个单词的随机文本，类似于图 [9-3](#Fig3) *中所示的示例查询输出。*确认 Description 列包含 8001 个单词，由于 Synapse Analytics 专用 SQL 池目标端的 8000 个字符的长度限制，这肯定会使 Azure 数据工厂管道失败，并且它将触发在`pipeline_errors`表中创建记录。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig3_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig3_HTML.jpg)

图 9-3

SSMS 的销售观。包含大量示例文本的地址描述列

## 向参数表中添加记录

在确定了要在流程中运行的源 SQL 表之后，将它们添加到`pipeline_parameter`表中，如图 [9-4](#Fig4) 所示。对于本练习，添加在上一步中创建的包含 8，001 个字符的大块的`SalesLT.Address`表，以及一个我们期望成功的常规表(例如`SalesLT.Customer`)，以演示成功和失败的端到端日志记录过程。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig4_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig4_HTML.jpg)

图 9-4

添加到 pipeline_parameter 表的记录的 SSMS 视图，用于测试 ADF 管道中的错误

## 验证 Azure 数据湖存储 Gen2 文件夹和文件

在运行管道将 SQL 表加载到 Azure Data Lake Storage Gen2 之后，图 [9-5](#Fig5) 显示目的地 ADLS Gen2 容器现在已经以 snappy compressed Parquet 格式拥有了这两个表。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig5_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig5_HTML.jpg)

图 9-5

包含来自 ADF 管道执行的文件夹的 ADLS Gen2 视图

作为额外的验证步骤，地址文件夹包含图 [9-6](#Fig6) 所示的预期拼花文件。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig6_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig6_HTML.jpg)

图 9-6

包含来自 ADF 管道执行的拼花文件的 ADLS Gen2 视图

## 配置管道查找活动

现在是构建和配置 ADF 管道的时候了。在第 [7](07.html) 章中， [I](01.html) 详细介绍了如何构建一个 ADF 管道来加载 Synapse Analytics 专用 SQL 池，其中包含存储在 ADLS Gen2 中的 parquet 文件。作为流程的回顾，查找中的 select 查询获取需要加载到 Synapse Analytics 专用 SQL 池的 parquet 文件列表，然后将它们传递到 ForEach 循环，该循环将 parquet 文件加载到 Synapse Analytics 专用 SQL 池，如图 [9-7](#Fig7) 所示。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig7_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig7_HTML.jpg)

图 9-7

ADF 管道查找活动设置和查询以获取列出的 pipeline_parameter 表

## 配置管道 ForEach 循环活动

ForEach 循环活动包含 Copy-Table 活动，该活动获取 parquet 文件并将它们加载到 Synapse Analytics 专用的 SQL 池中，同时自动创建表。如果复制表活动成功，它将把管道运行数据记录到`pipeline_log`表中。但是，如果 Copy-Table 活动失败，它会将管道错误详细信息记录到`pipeline_errors`表中。

## 配置存储过程来更新日志表

还要注意，之前创建的`UpdateLogTable`存储过程将被图 [9-8](#Fig8) 所示的成功存储过程活动调用。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig8_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig8_HTML.jpg)

图 9-8

ADF 管道存储过程设置

图 [9-9](#Fig9) 显示了将更新`pipeline_log`表并可直接从存储过程导入的存储过程参数。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig9_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig9_HTML.jpg)

图 9-9

ADF 管道存储过程参数

表 [9-1](#Tab1) 中列出的下列值需要作为存储过程活动参数值输入，如图 [9-9](#Fig9) 中部分所示。

表 9-1

要输入到管道日志存储过程活动的参数设置中的值列表

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

名字

 | 

价值

 |
| --- | --- |
| 数据工厂名称 | @{pipeline()。数据工厂} |
| 管道名称 | @{pipeline()。管道} |
| 运行 Id | @{pipeline()。runid} |
| 来源 | @{item()。src_name} |
| 目的地 | @{item().dst_name} |
| 触发类型 | @{pipeline()。triggertype} |
| 已触发 | @{pipeline()。triggerid} |
| TriggerName | @{pipeline()。triggername} |
| 触发时间 | @{pipeline()。triggertime} |
| 罗斯佩德 | @ { activity(' Copy-Table '). output . rowscopied } |
| RowsRead | @ { activity(' Copy-Table '). output . rows read } |
| No _ ParallelCopies | @ { activity(' Copy-Table '). output . usedparallelcopys } |
| 复制持续时间(秒) | @ { activity(' Copy-Table '). output . Copy duration } |
| 有效积分运行时间 | @ { activity(' Copy-Table '). output . effective integration runtime } |
| 来源类型 | @ { activity(' Copy-Table '). output . execution details[0]. source . type } |
| 汇 _ 类型 | @ { activity(' Copy-Table '). output . execution details[0]. sink . type } |
| 执行 _ 状态 | @ { activity(' Copy-Table '). output . execution details[0]。状态} |
| 复制活动开始时间 | @ { activity(' Copy-Table '). output . execution details[0]。开始} |
| 复制活动结束时间 | @{utcnow()} |
| 复制活动 _ 队列持续时间 _ 秒 | @ { activity(' Copy-Table '). output . execution details[0]. detailed durations . queuingduration } |
| 复制活动 _ 传输持续时间 _ 秒 | @ { activity(' Copy-Table '). output . execution details[0]. detailed durations . transfer duration } |

## 配置一个存储过程来更新错误表

ForEach 循环活动中的最后一个存储过程是之前创建的`UpdateErrorTable`存储过程，将被失败存储过程活动调用，如图 [9-10](#Fig10) 所示。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig10_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig10_HTML.jpg)

图 9-10

错误的 ADF 管道存储过程

图 [9-11](#Fig11) 显示了将更新`pipeline_errors`表的存储过程参数，这些参数可以直接从存储过程中导入。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig11_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig11_HTML.jpg)

图 9-11

错误的 ADF 管道存储过程参数

表 [9-2](#Tab2) 中的以下值需要作为存储过程参数值输入，如图 [9-11](#Fig11) 中部分所示。

表 9-2

要输入到管道错误存储过程活动的参数设置中的值列表

<colgroup><col class="tcol1 align-left"> <col class="tcol2 align-left"></colgroup> 
| 

名字

 | 

价值

 |
| --- | --- |
| 数据工厂名称 | @{pipeline()。数据工厂} |
| 管道名称 | @{pipeline()。管道} |
| 运行 Id | @{pipeline()。runid} |
| 来源 | @{item()。src_name} |
| 目的地 | @{item().dst_name} |
| 触发类型 | @{pipeline()。triggertype} |
| 已触发 | @{pipeline()。triggerid} |
| TriggerName | @{pipeline()。triggername} |
| 触发时间 | @{pipeline()。triggertime} |
| No _ ParallelCopies | @ { activity(' Copy-Table '). output . usedparallelcopys } |
| 复制持续时间(秒) | @ { activity(' Copy-Table '). output . Copy duration } |
| 有效积分运行时间 | @ { activity(' Copy-Table '). output . effective integration runtime } |
| 来源类型 | @ { activity(' Copy-Table '). output . execution details[0]. source . type } |
| 汇 _ 类型 | @ { activity(' Copy-Table '). output . execution details[0]. sink . type } |
| 执行 _ 状态 | @ { activity(' Copy-Table '). output . execution details[0]。状态} |
| 错误代码 | @ { activity(' Copy-Table '). error . error code } |
| 错误描述 | @ { activity(' Copy-Table '). error . message } |
| 错误日志时间 | @utcnow() |
| 故障类型 | @ concat(activity(' Copy-Table '). error . message，' failuretype:'，activity(' Copy-Table '). error . failure type)。 |

## 运行管道

现在您已经配置了管道，继续运行管道。从图 [9-12](#Fig12) 中的调试模式输出日志中可以看出，正如所料，一个表成功，另一个表失败。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig12_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig12_HTML.jpg)

图 9-12

包含活动状态的 ADF 管道运行输出

## 核实结果

最后，验证`pipeline_log`表中的结果。注意在图 [9-13](#Fig13) 中`pipeline_log`表已经捕获了一个包含源`SalesLT.Customer`的日志。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig13_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig13_HTML.jpg)

图 9-13

pipeline_log 表的 SSMS 视图显示成功的 ADF 管道

`pipeline_errors`表中有一条`SalesLT.Address`的记录，以及详细的错误代码、描述、消息等，如图 [9-14](#Fig14) 所示。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig14_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig14_HTML.jpg)

图 9-14

pipeline_errors 表的 SSMS 视图，显示失败的 ADF 管道

作为最后一项检查，在导航到 Synapse Analytics 专用 SQL 池时，请注意图 [9-15](#Fig15) 中的两个表都是自动创建的，尽管一个失败，一个成功。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig15_HTML.png](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig15_HTML.png)

图 9-15

自动创建的 Synapse Analytics 专用 SQL 池表的视图

值得注意的是，由于`etl.Address`发生故障，数据仅加载到`etl.Customer`中，随后没有数据加载到其中。图 [9-16](#Fig16) 显示了来自`etl.Address`的数据的`select *`，以确认表格不包含任何数据。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig16_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig16_HTML.jpg)

图 9-16

选择*自动创建的 etl。地址表，由于 ADF 管道失败，该表不包含任何数据

## 其他 ADF 日志记录选项

前面几节描述了在 ADF 管道中记录和处理错误的一种更加自定义的方法。在图 [9-17](#Fig17) 所示的复制数据活动中，有一个验证数据一致性、管理容错和启用日志记录的内置方法。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig17_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig17_HTML.jpg)

图 9-17

ADF 复制数据活动中的数据验证、容错和日志记录选项

以下列表包含 ADF 中“复制数据”活动的“设置”选项卡中作为可选配置属性提供的功能的详细信息和定义。这些功能旨在提供额外的现成数据验证、确认和日志记录方法，并且可以在 ADF 管道中有选择地启用和/或禁用它们:

*   **数据一致性验证**:选择此选项时，复制活动将在数据移动后在源和目标存储之间进行额外的数据一致性验证。验证包括二进制文件的文件大小检查和校验和验证，以及表格数据的行数验证。

*   **容错**:选择此选项时，您可以忽略复制过程中间发生的一些错误，例如，源存储和目标存储之间的不兼容行、数据移动过程中文件被删除等。

*   **启用日志记录**:选择此选项时，您可以将复制的文件、跳过的文件和行记录到 blob 存储中。

类似地，在映射数据流中，sink 设置中也有一个选项，在图 [9-18](#Fig18) 的情况下，它使用 Azure SQL DB。请注意各种选项，包括错误行处理、事务提交和报告错误成功选项。

![../images/511918_1_En_9_Chapter/511918_1_En_9_Fig18_HTML.jpg](../images/511918_1_En_9_Chapter/511918_1_En_9_Fig18_HTML.jpg)

图 9-18

ADF 映射数据流中的错误行处理

以下列表包含 ADF 中接收器复制数据活动的“设置”选项卡中作为可选配置属性提供的功能的详细信息和定义。这些特性旨在提供额外的开箱即用错误处理、事务提交等，并且可以在 ADF 管道中有选择地启用和/或禁用它们:

*   **错误行处理**:“出错时继续”将防止管道失败——例如，在加载数百万行时，有几行在过程中失败。

*   **事务提交**:在 SQL 事务提交的上下文中允许“单个”和“批量”提交。例如，批处理将具有更好的性能，因为数据将成批加载，而不是单次提交。

*   **输出被拒绝的数据**:此选项类似于常规的 ADF 复制数据活动，选择后会提示您将被拒绝的数据记录到存储帐户。

*   **出错时报告成功**:选中时，管道出错时报告成功。

## 摘要

在这一章中，我介绍了自定义 ADF 日志框架中的`pipeline_errors`表，并讨论了它与前面章节中创建的`pipeline_parameter`和`pipeline_log`表的相关性和关系。此外，我还演示了如何在 ADF ELT 管道中设计和实现一个流程，通过在源系统上重新创建一个错误，并跟踪失败活动的 ADF 管道执行流程如何处理错误，从而将错误详细信息记录到`pipeline_errors`表中。最后，我描述了其他一些内置方法，用于管理 ADF 的复制数据活动和映射数据流中的错误。