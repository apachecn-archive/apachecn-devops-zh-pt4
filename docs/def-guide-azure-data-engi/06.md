# 6.将数据湖存储 Gen2 文件加载到 Synapse Analytics 专用 SQL 池中

第 [4](04.html) 章向您展示了如何创建一个动态的、参数化的、元数据驱动的流程，将数据从本地 SQL Server 完全加载到 Azure Data Lake Storage Gen2。本章将演示如何将所有来自 ADLS Gen2 的 snappy 压缩拼花数据文件完全加载到 Azure 专用 SQL 池中。

Azure Data Factory 的 sink Copy 活动允许三种不同的复制方法将数据加载到 Azure 专用 SQL 池中，这是 Azure Synapse 分析生态系统的一部分。本章将使用动态和参数化的 ADF 管道来探索这三种方法:

1.  聚合碱

2.  复制命令

3.  批量插入

首先，要做一些准备工作，创建本章前几节中演示的数据集和管道。在本章的最后，将讨论和演示前面列出的三种加载数据的方法。

## 重新创建管道参数表

首先在 ADF_DB 中重新创建您在第 [4 章](04.html)中创建的`pipeline_parameter`表，使其更加健壮，为本章将要构建的 ADF 管道做准备。

下面是重新创建该表的代码。如果该表存在，它首先删除该表。然后，它重新创建表:

```
USE [ADF_DB]

go

/****** Object:  Table [dbo].[pipeline_parameter]    ******/
IF EXISTS (SELECT *
           FROM   sys.objects
           WHERE  object_id = Object_id(N'[dbo].[pipeline_parameter]')
                  AND type IN ( N'U' ))
  DROP TABLE [dbo].[pipeline_parameter]

go

/****** Object:  Table [dbo].[pipeline_parameter]  ******/
SET ansi_nulls ON

go

SET quoted_identifier ON

go

CREATE TABLE [dbo].[pipeline_parameter]
  (
     [parameter_id]                       [INT] IDENTITY(1, 1) NOT NULL,
     [server_name]                        [NVARCHAR](500) NULL,
     [src_type]                           [NVARCHAR](500) NULL,
     [src_schema]                         [NVARCHAR](500) NULL,
     [src_db]                             [NVARCHAR](500) NULL,
     [src_name]                           [NVARCHAR](500) NULL,
     [dst_type]                           [NVARCHAR](500) NULL,
     [dst_schema]                         [NVARCHAR](500) NULL,
     [dst_name]                           [NVARCHAR](500) NULL,
     [include_pipeline_flag]              [NVARCHAR](500) NULL,
     [partition_field]                    [NVARCHAR](500) NULL,
     [process_type]                       [NVARCHAR](500) NULL,
     [priority_lane]                      [NVARCHAR](500) NULL,
     [pipeline_date]                      [NVARCHAR](500) NULL,
     [pipeline_status]                    [NVARCHAR](500) NULL,
     [load_synapse]                       [NVARCHAR](500) NULL,
     [load_frequency]                     [NVARCHAR](500) NULL,
     [dst_folder]                         [NVARCHAR](500) NULL,
     [file_type]                          [NVARCHAR](500) NULL,
     [lake_dst_folder]                    [NVARCHAR](500) NULL,
     [spark_flag]                         [NVARCHAR](500) NULL,
     [dst_schema]                         [NVARCHAR](500) NULL,
     [distribution_type]                  [NVARCHAR](500) NULL,
     [load_sqldw_etl_pipeline_date]       [DATETIME] NULL,
     [load_sqldw_etl_pipeline_status]     [NVARCHAR](500) NULL,
     [load_sqldw_curated_pipeline_date]   [DATETIME] NULL,
     [load_sqldw_curated_pipeline_status] [NVARCHAR](500) NULL,
     [load_delta_pipeline_date]           [DATETIME] NULL,
     [load_delta_pipeline_status]         [NVARCHAR](500) NULL,
     PRIMARY KEY CLUSTERED ( [parameter_id] ASC )WITH (statistics_norecompute =
     OFF, ignore_dup_key = OFF) ON [PRIMARY]
  )
ON [PRIMARY]

go

```

从代码中列出的列可以看出，已经添加了相当多的新元数据字段，可以通过创建动态数据集和管道在 ADF 管道中捕获这些字段。

## 创建数据集

在下一节中，为 ADLS Gen2 snappy 压缩拼花文件创建一个源数据集，为 Azure 专用 SQL 池创建一个接收数据集。

首先创建三个数据集，并将数据集命名如下:

1.  ADLS 到突触

2.  ADLS 至突触

3.  DS_SYNAPSE_ANALYTICS_DW

接下来的小节将展示如何创建它们。

### ADLS 到突触

首先创建一个带有参数化路径的源 ADLS Gen2 数据集。请记住，`pipeline_date`已经添加到您在第 [4](04.html) 章中创建的`pipeline_parameter`表中，因为`pipeline_date`捕获了数据加载到 ADLS Gen2 的日期。在此步骤中，您将从 ADLS Gen2 向 Synapse Analytics 专用 SQL 池加载数据。您可以从第 [4 章](04.html)重新运行管道，或者在此`pipeline_date`栏中手动输入日期，该日期最好包含最新的文件夹日期。第 [8](08.html) 章将讨论如何自动将最大文件夹日期插入到此`pipeline_date`列中，以确保此列始终具有可以传递到参数化 ADF 管道中的最新和最大文件夹日期。这将通过使用在复制活动成功后立即运行的存储过程活动来实现。

图 [6-1](#Fig1) 说明了如何设置参数化连接属性，以读取由`pipeline_parameter`控制表驱动的源 ADLS Gen2 拼花目录和文件夹。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig1_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig1_HTML.jpg)

图 6-1

ADLS Gen2 拼花文件夹和文件的 ADF 参数化连接

以下是添加到图 [6-1](#Fig1) 中文件路径部分的代码:

```
@{item().dst_folder}

@{item().dst_name}/parquet/ @{item().pipeline_date}/ @{item().dst_name}.parquet

```

图 [6-2](#Fig2) 显示了如何添加所需的参数。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig2_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig2_HTML.jpg)

图 6-2

ADLS Gen2 实木复合地板文件夹和文件的 ADF 参数

图 [6-3](#Fig3) 中说明了链接的服务细节。Azure 密钥库用于存储凭据机密。当管道开始运行时，如果发现任何身份验证错误，这将与后面的部分相关。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig3_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig3_HTML.jpg)

图 6-3

使用 Azure Key Vault 的 ADF 链接服务连接属性

### ADLS 至突触

图 [6-4](#Fig4) 中的 ADF 数据集连接使用托管身份连接凭据。图 [6-4](#Fig4) 中显示的数据集与上一个数据集的区别在于，这个链接的服务连接不使用 Azure Key Vault。稍后发现错误时，使用此选项来测试密钥存储库连接和非密钥存储库连接并在两者之间切换。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig4_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig4_HTML.jpg)

图 6-4

使用托管身份的 ADF 数据集连接属性

以下是添加到图 [6-4](#Fig4) 中文件路径部分的代码:

```
@{item().dst_folder}

@{item().dst_name}/parquet/ @{item().pipeline_date}/ @{item().dst_name}.parquet

```

与之前的数据集类似，添加如图 [6-5](#Fig5) 所示的参数。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig5_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig5_HTML.jpg)

图 6-5

ADLS 第二代实木复合地板文件夹和文件的 ADF 参数-托管身份

链接的服务详情如图 [6-6](#Fig6) 所示。这里没有使用 Azure Key Vault。同样，当执行管道时，如果发现任何身份验证错误，这将与后面的部分相关。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig6_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig6_HTML.jpg)

图 6-6

ADF 链接服务连接

在此部分中，已使用托管身份创建了新的 ADF 链接服务连接。

### DS_SYNAPSE_ANALYTICS_DW

接收器连接将连接到 Azure Synapse Analytics 专用的 SQL 池，如图 [6-7](#Fig7) 所示。此外，参数用于指定来自`pipeline_parameter`表的模式和表名。当 ForEach 循环活动将用于使用同一个接收数据集创建多个表时，这将是一个很好的特性。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig7_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig7_HTML.png)

图 6-7

ADF Synapse DW 链接的服务连接属性

以下是添加到图 [6-7](#Fig7) 中文件路径部分的代码:

```
@{item().src_schema}

@{item().dst_name}

```

## 创建管道

既然已经创建了数据集，那么还要创建一个新的管道。这样做时，添加一个连接到 ForEach 循环活动的查找活动，如图 [6-8](#Fig8) 所示。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig8_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig8_HTML.png)

图 6-8

包含查找和 ForEach 循环活动的 ADF 管道画布

图 [6-9](#Fig9) 中显示的查找查询将获得需要加载到 Azure Synapse Analytics 专用 SQL 池的表列表。请注意，目前有一个过滤器应用于查询，它将只包括`load_synapse` = 1 的记录。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig9_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig9_HTML.jpg)

图 6-9

ADF 查找活动查询设置

图 [6-9](#Fig9) 中包含的代码片段如下:

```
SELECT [server_name],
       [src_type],
       [src_schema],
       [src_db],
       [src_name],
       [dst_type],
       [dst_name],
       [include_pipeline_flag],
       [partition_field],
       [process_type],
       [priority_lane],
       [pipeline_date],
       [pipeline_status],
       [dst_folder],
       [file_type]
FROM   [dbo].[pipeline_parameter]
WHERE  load_synapse = 1

```

在 ForEach 循环活动的设置中，添加查找活动的输出值，如图 [6-10](#Fig10) 所示。记住不要选中“顺序”框，以确保多个表并行处理。如果留空，默认“批次计数”为 20，最大值为 50。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig10_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig10_HTML.png)

图 6-10

ADF ForEach 活动设置

还要在 ForEach 循环活动中添加一个 Copy 活动，如图 [6-11](#Fig11) 所示。单击铅笔图标查看复制活动。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig11_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig11_HTML.png)

图 6-11

ADF ForEach 活动

源设置为 DS_ADLS_TO_SYNAPSE，它在链接的服务连接中使用 Azure 密钥库。添加所需的动态参数。请注意，参数是在数据集中定义的。图 [6-12](#Fig12) 显示了如何以及在哪里添加这些值。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig12_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig12_HTML.jpg)

图 6-12

ADF 复制活动源数据集属性

最后，选择 DS_SYNAPSE_ANALYTICS_DW 数据集作为 sink，并在启用“自动创建表”选项的情况下选择“批量插入”，如图 [6-13](#Fig13) 所示。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig13_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig13_HTML.png)

图 6-13

ADF 复制活动接收器数据集属性

基于管道的当前配置，由于它是由`pipeline_parameter`表驱动的，当(n)个表/记录被添加到管道参数表并且`load_synapse`标志被设置为 1 时，管道将基于所选择的复制方法并行地执行和加载所有表到 Azure Synapse Analytics 专用 SQL 池。

## 选择复制方法

现在，我们终于到了选择复制方法的时候了。接收器复制方法有三个选项。批量插入、聚合库和复制命令都是您将在本节中学习使用的选项。

### 批量插入

SQL Server 提供了 BULK INSERT 语句，以便使用 T-SQL 高效、快速地将大量数据导入 SQL Server，并且只需最少的日志记录操作。

在 ADF 复制活动的 Sink 选项卡中，将复制方法设置为 [Bulk](https://docs.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql%253Fview%253Dsql-server-ver15) insert。如果表不存在，则“自动创建表”会使用源文件中的模式自动创建该表。当接收器指定存储过程或复制活动配备了分段设置时，不支持这种情况。对于这个场景，源文件是一个 parquet snappy 压缩文件，不包含 VARCHAR(MAX)等不兼容的数据类型，因此“自动创建表”选项应该没有问题。

请注意，预复制脚本将在创建表之前运行，因此在使用“自动创建表”的场景中，当表不存在时，首先在没有预复制脚本的情况下运行它，以防止出现错误，然后在为正在进行的完整加载创建表之后，再添加预复制脚本。图 [6-14](#Fig14) 显示了接收器设置以及添加任何预拷贝脚本(如截断脚本)的位置。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig14_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig14_HTML.png)

图 6-14

ADF 复制活动接收器复制前脚本

如果默认的“自动创建表”选项不能满足基于表的自定义分发的分发需求，那么可以利用“添加动态内容”来使用在每个表的管道参数表中指定的分发方法。

以下是添加到图 [6-14](#Fig14) 中的预拷贝脚本部分的代码:

```
TRUNCATE TABLE @{item().src_schema}.@{item().dst_name}

```

运行管道后，使用批量插入复制方法成功，如图 [6-15](#Fig15) 所示的活动运行监视器所示。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig15_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig15_HTML.png)

图 6-15

大容量插入的 ADF 管道成功

图 [6-16](#Fig16) 显示了批量插入复制管道状态的详细信息。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig16_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig16_HTML.jpg)

图 6-16

大容量插入的 ADF 管道运行详细信息

查询 Synapse 表后，注意表中有相同数量的行，如图 [6-17](#Fig17) 所示。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig17_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig17_HTML.jpg)

图 6-17

查询 Synapse Analytics 专用 SQL 池表以验证批量插入 ADF 管道结果

批量插入方法也适用于内部 SQL Server 作为源，Synapse Analytics 专用 SQL 池作为接收器。

### 聚合碱

使用 [PolyBase](https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide) 是以高吞吐量将大量数据加载到 Azure Synapse Analytics 的有效方式。通过使用 PolyBase 代替默认的 Bulk insert 机制，您将会看到吞吐量的大幅提高。

在下一个练习中，选择 [PolyBase](https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide%253Fview%253Dsql-server-ver15) ，如图 [6-18](#Fig18) 所示，以测试该复制方法。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig18_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig18_HTML.png)

图 6-18

用于选择聚合库的 ADF 管道接收器数据集属性

PolyBase 将需要托管身份凭证来提供 Azure AD 并授予数据工厂对数据库的完全访问权限。

有关验证访问的更多详细信息，请查看并在 Synapse Analytics 专用 SQL 池上运行以下查询:

```
select * from sys.database_scoped_credentials
select * from sys.database_role_members
select * from sys.database_principals

```

此外，当需要创建外部表、数据源和文件格式时，以下查询有助于验证所需的对象是否已创建:

```
select * from sys.external_tables
select * from sys.external_data_sources
select * from sys.external_file_formats

```

配置并运行管道后，您可能会注意到管道失败，并出现以下错误:

```
"ErrorCode=FailedDbOperation,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=Error happened when loading data into SQL Data Warehouse.,Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=External file access failed due to internal error: 'Error occurred while accessing HDFS: Java exception raised on call to HdfsBridge_IsDirExist. Java exception message:\r\nHdfsBridge::isDirExist - Unexpected error encountered checking whether directory exists or not: AbfsRestOperationException: Operation failed: \"This request is not authorized to perform this operation.\", 403, HEAD, https://lake.dfs.core.windows.net/lake     //?upn=false&action=getAccessControl&timeout=90',Source=.Net SqlClient Data Provider,SqlErrorNumber=105019,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=105019,State=1,Message=External file access failed due to internal error: 'Error occurred while accessing HDFS: Java exception raised on call to HdfsBridge_IsDirExist. Java exception message:\r\nHdfsBridge::isDirExist - Unexpected error encountered checking whether directory exists or not: AbfsRestOperationException: Operation failed: \"This request is not authorized to perform this operation.\", 403, HEAD, https://lake.dfs.core.windows.net/lake     //?upn=false&action=getAccessControl&timeout=90',},],'",

```

在研究错误后，原因是因为源数据集 DS_ADLS_TO_SYNAPSE 的原始 Azure 数据湖存储链接服务正在使用 Azure 密钥库来存储身份验证凭据，这是目前不支持的托管身份验证方法，用于使用 PolyBase 和 Copy 命令。

将源数据集更改为 DS_ADLS_TO_SYNAPSE_MI，它不再使用 Azure 密钥库，请注意图 [6-19](#Fig19) 中管道使用 PolyBase 复制方法成功。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig19_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig19_HTML.jpg)

图 6-19

更改为托管身份后，ADF 管道执行显示成功

### 复制命令

[Copy](https://docs.microsoft.com/en-us/sql/t-sql/statements/copy-into-transact-sql%253Fview%253Dazure-sqldw-latest) 命令的功能与 PolyBase 相似，因此 PolyBase 所需的权限对于 Copy 命令来说也是绰绰有余的。有关复制到的更多信息，请重新访问第 [5](05.html) 章，其中涵盖了权限、用例以及复制到的 SQL 语法的详细信息。图 [6-20](#Fig20) 显示了如何在 ADF 接收活动中配置复制命令。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig20_HTML.png](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig20_HTML.png)

图 6-20

在 ADF 接收器中配置复制命令

与使用 Azure Key Vault 的 PolyBase 复制方法类似，您会注意到以下略有不同的错误消息:

```
ErrorCode=UserErrorSqlDWCopyCommandError,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=SQL DW Copy Command operation failed with error 'Not able to validate external location because The remote server returned an error: (403) Forbidden.',Source=Microsoft.DataTransfer.ClientLibrary,''Type=System.Data.SqlClient.SqlException,Message=Not able to validate external location because The remote server returned an error: (403) Forbidden.,Source=.Net SqlClient Data Provider,SqlErrorNumber=105215,Class=16,ErrorCode=-2146232060,State=1,Errors=[{Class=16,Number=105215,State=1,Message=Not able to validate external location because The remote server returned an error: (403) Forbidden.,},],'", "failureType": "UserError", "target": "Copy data1", "details": []

```

切换到不使用 Azure Key Vault 的链接服务，注意管道成功，如图 [6-21](#Fig21) 所示。

![../images/511918_1_En_6_Chapter/511918_1_En_6_Fig21_HTML.jpg](../images/511918_1_En_6_Chapter/511918_1_En_6_Fig21_HTML.jpg)

图 6-21

ADF 管道执行结果

请注意，一旦创建并测试了 ADF 管道，考虑调度和触发它们是很重要的。触发器根据触发器类型和触发器中定义的标准确定何时触发管道执行。有三种主要类型的 Azure 数据工厂触发器:根据挂钟时间表执行管道的**时间表**触发器，定期执行管道并保持管道状态的**翻转窗口**触发器，以及响应 blob 相关事件的基于**事件的**触发器。此外，ADF 具有警报功能，可监控管道和触发故障，并通过电子邮件、文本等发送通知。

## 摘要

在本章中，我向您展示了如何创建源 Azure 数据湖存储 Gen2 数据集和 sink Synapse Analytics 专用 SQL 池数据集，以及由参数表驱动的 Azure 数据工厂管道。您还学习了如何使用三种复制方法将 snappy 压缩的 parquet 文件加载到 Synapse Analytics 专用的 SQL 池中:批量插入、聚合库和复制命令。本章向您介绍了 Azure Data Factory 中可用的各种摄取选项。