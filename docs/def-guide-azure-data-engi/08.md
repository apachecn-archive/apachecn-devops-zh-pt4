# 8.在 SQL 数据库中为管道活动指标构建自定义日志

在前面的章节中，我演示了如何使用数据工厂将数据从 ADLS Gen2 加载到 Synapse Analytics 专用的 SQL 池中。在这一章中，我将演示如何利用已经建立的管道来实现一个过程，该过程用于跟踪运行和保存数据的管道的日志活动。

Azure Data Factory 是一个健壮的基于云的 ELT 工具，能够适应记录管道审计数据的多种场景，包括现成的服务，如日志分析、Azure Monitor 等，以及提取管道指标并将它们传递给另一个自定义流程的更多自定义方法。在本章中，我将向您展示如何实现这些可能的自定义日志记录选项中的三个，它们是:

1.  **选项 1–创建存储过程活动**:使用 ADF 更新静态管道参数表中的管道状态和日期时间列可以通过使用存储过程活动来实现。

2.  **选项 2–在 Data Lake Storage Gen2** 中创建 CSV 日志文件:为创建的每个拼花文件生成一个元数据 CSV 文件，并将日志作为 CSV 文件存储在 ADLS Gen2 中的分层文件夹中，这可以使用复制数据活动来实现。

3.  **选项 3——在 Azure SQL 数据库中创建日志表**:在 Azure SQL 数据库中创建管道日志表，并将管道活动作为记录存储在表中，可以通过使用 Copy data activity 来实现。

图 [8-1](#Fig1) 说明了这三个选项，并显示了从复制表活动到创建各种记录方法的数据流的可视化表示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig1_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig1_HTML.jpg)

图 8-1

用于记录自定义管道数据的 ADF 选项

## 选项 1:创建一个存储过程活动

存储过程活动将用于调用 Synapse Analytics 专用 SQL 池中的存储过程。

对于这个场景，您可能希望将您的`pipeline_status`和`pipeline_date`细节作为列保存在您的`adf_db.dbo.pipeline_parameter`表中，而不是拥有一个单独的日志表。这种方法的缺点是，它不会保留历史日志数据，而只是根据对传入文件在`pipeline_parameter`表中的记录的查找来更新原始`pipeline_parameter`表中的值。这提供了一种快速但不一定可靠的方法来查看管道参数表中所有项目的状态和加载日期。这种方法的具体用例可能是根据传入的文件夹和文件名以及时间戳来记录最大文件夹日期。

首先，在 ForEach 循环活动中添加一个存储过程活动，如图 [8-2](#Fig2) 所示，以确保流程使用存储过程迭代并记录每个表。请注意绿色箭头和线连接器，它指示存储过程必须在复制数据活动成功时运行。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig2_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig2_HTML.jpg)

图 8-2

使用存储过程记录数据的 ADF 选项 1

接下来，将以下存储过程添加到管道参数表所在的数据库中。此过程只是在管道参数表中查找目标表名，并在复制数据活动成功后更新每个表的状态和日期时间。类似地，通过在复制活动完成后将存储过程链接到失败约束，您可以轻松地添加一个新的存储过程来处理错误:

```
SET quoted_identifier ON

go

CREATE PROCEDURE [dbo].[Sql2adls_data_files_loaded] @dst_name NVARCHAR(500)
AS
    SET nocount ON
    -- turns off messages sent back to client after DML is run, keep this here

    DECLARE @Currentday DATETIME = Getdate();

    UPDATE [dbo].[pipeline_parameter]
    SET    pipeline_status = 'success',
           pipeline_date = @Currentday
    WHERE  dst_name = @dst_name;

go

```

创建存储过程后，确认它已在相应的数据库中创建。注意图 [8-3](#Fig3) 中的确认，这是 SSMS 境内的屏幕截图。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig3_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig3_HTML.jpg)

图 8-3

显示 ADF 选项 1 使用的存储过程的 SSMS 视图

接下来，返回到数据工厂管道并配置存储过程活动。在“存储过程”选项卡中，选择刚刚创建的存储过程。还要添加一个新的存储过程参数，该参数引用在复制活动中配置的目的地名称，如图 [8-4](#Fig4) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig4_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig4_HTML.jpg)

图 8-4

ADF 存储过程详细信息和参数

在保存、发布和运行管道之后，注意到`pipeline_date`和`pipeline_status`列已经作为 ADF 存储过程活动的结果被更新，如图 [8-5](#Fig5) 中的`pipeline_parameter`表视图所示。这是一个轻量级的测试模式，在一个集中的位置为您提供每个表的状态和加载日期的详细信息。我注意到 ADF 并不总是提供与有问题的表或列相关的健壮细节。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig5_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig5_HTML.jpg)

图 8-5

ADF 存储过程中更新的 pipeline_date 和 pipeline_status 列的 SSMS 视图

## 选项 2:在数据湖存储 Gen2 中创建一个 CSV 日志文件

由于您的复制表活动正在将快速拼花文件生成到分层的 ADLS Gen2 文件夹中，您可能还希望创建一个 CSV 文件，其中包含您的 ADLS Gen2 帐户中每个拼花文件的管道运行详细信息。对于这个场景，您可以设置一个数据工厂事件网格触发器来监听元数据文件，然后触发一个流程来转换源表并将其加载到一个管理区域中，从而在某种程度上复制一个接近实时的 ELT 流程。

开始设计 ADF 管道，通过添加用于创建日志文件的复制活动并将其连接到 Copy-Table 活动，在您的 ADLS Gen2 帐户中创建 CSV 日志文件，如图 [8-6](#Fig6) 所示。与上一个遍历每个表的过程类似，该过程将在每个表的元数据文件夹中生成一个 CSV 扩展元数据文件。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig6_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig6_HTML.jpg)

图 8-6

ADF 选项 2 用于记录数据，使用复制数据活动创建 CSV 文件

若要配置源数据集，请选择源本地 SQL Server。接下来，添加如图 [8-7](#Fig7) 所示的查询作为源查询。请注意，该查询将包含管道活动、复制表活动和用户定义参数的组合。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig7_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig7_HTML.jpg)

图 8-7

ADF 选项 2 用于提取管道活动指标的源查询

***回想一下前面章节中的注释，这些注释指出将源查询作为代码嵌入到 ADF 管道中仅仅是为了直观演示的目的。将源查询作为存储过程添加，然后调用存储过程，这总是一个更好、更高效的过程。这将有助于在一个集中的位置维护代码。**T3】*

下面是图 [8-7](#Fig7) 中使用的相应源代码:

```
SELECT '@{pipeline().DataFactory}'                     AS datafactory_name,
       '@{pipeline().Pipeline}'                        AS pipeline_name,
       '@{pipeline().RunId}'                           AS runid,
       '@{item().src_name}'                            AS source,
       '@{item().dst_name}'                            AS destination,
       '@{pipeline().TriggerType}'                     AS triggertype,
       '@{pipeline().TriggerId}'                       AS triggerid,
       '@{pipeline().TriggerName}'                     AS triggername,
       '@{pipeline().TriggerTime}'                     AS triggertime,
       '@{activity('copy-TABLE').output.rowsCopied}'   AS rowscopied,
       '@{activity('copy-TABLE').output.rowsRead}'     AS rowsread,
       '@{activity('copy-TABLE').output.usedParallelCopies}'                                      AS no_parallelcopies,
       '@{activity('copy-TABLE').output.copyDuration}'                                          AS copyduration_in_secs,
       '@{activity('copy-TABLE').output.effectiveIntegrationRuntime}'         AS effectiveintegrationruntime,
       '@{activity('copy-TABLE').output.executionDetails[0].source.type}'                AS source_type,
       '@{activity('copy-TABLE').output.executionDetails[0].sink.type}'                  AS sink_type,
       '@{activity('copy-TABLE').output.executionDetails[0].status}'                     AS execution_status,
       '@{activity('copy-TABLE').output.executionDetails[0].start}'                      AS copyactivity_start_time,
       '@{utcnow()}'                AS copyactivity_end_time,
       '@{activity('copy-TABLE').output.executionDetails[0].detailedDurations.queuingDuration}'  AS copyactivity_queuingduration_in_secs,
       '@{activity('copy-TABLE').output.executionDetails[0].detailedDurations.timeToFirstByte}'  AS copyactivity_timetofirstbyte_in_secs,
       '@{activity('copy-TABLE').output.executionDetails[0].detailedDurations.transferDuration}' AS copyactivity_transferduration_in_secs

```

sink 将是一个 CSV 数据集，扩展名为 CSV，如图 [8-8](#Fig8) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig8_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig8_HTML.jpg)

图 8-8

CSV 的 ADF 接收器数据集

图 [8-9](#Fig9) 显示了用于 CSV 数据集的连接配置。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig9_HTML.png](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig9_HTML.png)

图 8-9

CSV 的 ADF 接收器数据集连接属性

以下参数化路径将确保在正确的文件夹结构中生成文件。下面是如图 [8-9](#Fig9) 所示的代码:

```
@{item().server_name}/@{item().src_db}/@{item().src_schema}/@{item().dst_name}/metadata/@{formatDateTime(utcnow(),'yyyy-MM-dd')}/@{item().dst_name}.csv

```

保存、发布和运行管道后，请注意元数据文件夹是如何在以下文件夹结构中创建的:

```
Server>database>schema>date>Destination_table location

```

图 [8-10](#Fig10) 显示了您在 ADSL Gen2 中看到的这个文件夹。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig10_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig10_HTML.jpg)

图 8-10

ADF 管道选项 2 生成的 ADLS Gen2 文件夹

打开元数据文件夹，注意管道运行的每一天都会创建 CSV 文件夹，如图 [8-11](#Fig11) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig11_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig11_HTML.jpg)

图 8-11

由 ADF 管道选项 2 创建的 ADLS Gen2 文件夹(按时间戳)

最后，请注意图 [8-12](#Fig12) 中的元数据。以该表的名称命名的 csv 文件已创建。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig12_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig12_HTML.jpg)

图 8-12

包含来自选项 2 的 ADF 管道指标的 ADLS Gen2 元数据文件

下载并打开文件，注意所有的查询结果都已经被填充到。csv 文件，如图 [8-13](#Fig13) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig13_HTML.png](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig13_HTML.png)

图 8-13

ADLS Gen2 元数据文件，包含选项 2 中 ADF 管道指标的详细信息

## 选项 3:在 Azure SQL 数据库中创建日志表

本章的最后一个场景是在参数表所在的数据库中创建一个日志表，然后将数据作为记录写入该表。对于该选项，首先添加一个连接到复制表活动的复制数据活动，如图 [8-14](#Fig14) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig14_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig14_HTML.jpg)

图 8-14

ADF 选项 3，用于使用写入日志表的复制数据活动记录数据

接下来，在 ADF_DB 数据库中创建下表。此表将存储和捕获管道和复制活动的详细信息:

```
SET ansi_nulls ON

go

SET quoted_identifier ON

go

CREATE TABLE [dbo].[pipeline_log]
  (
     [log_id]                                [INT] IDENTITY(1, 1) NOT NULL,
     [parameter_id]                          [INT] NULL,
     [datafactory_name]                      [NVARCHAR](500) NULL,
     [pipeline_name]                         [NVARCHAR](500) NULL,
     [runid]                                 [NVARCHAR](500) NULL,
     [source]                                [NVARCHAR](500) NULL,
     [destination]                           [NVARCHAR](500) NULL,
     [triggertype]                           [NVARCHAR](500) NULL,
     [triggerid]                             [NVARCHAR](500) NULL,
     [triggername]                           [NVARCHAR](500) NULL,
     [triggertime]                           [NVARCHAR](500) NULL,
     [rowscopied]                            [NVARCHAR](500) NULL,
     [dataread]                              [INT] NULL,
     [no_parallelcopies]                     [INT] NULL,
     [copyduration_in_secs]                  [NVARCHAR](500) NULL,
     [effectiveintegrationruntime]           [NVARCHAR](500) NULL,
     [source_type]                           [NVARCHAR](500) NULL,
     [sink_type]                             [NVARCHAR](500) NULL,
     [execution_status]                      [NVARCHAR](500) NULL,
     [copyactivity_start_time]               [NVARCHAR](500) NULL,
     [copyactivity_end_time]                 [NVARCHAR](500) NULL,
     [copyactivity_queuingduration_in_secs]  [NVARCHAR](500) NULL,
     [copyactivity_transferduration_in_secs] [NVARCHAR](500) NULL,
     CONSTRAINT [PK_pipeline_log] PRIMARY KEY CLUSTERED ( [log_id] ASC )WITH (
     statistics_norecompute = OFF, ignore_dup_key = OFF) ON [PRIMARY]
  )
ON [PRIMARY]

go

```

与最后一个管道选项类似，将本地 SQL Server 配置为源，并将选项 2 中提供的查询代码用作源查询，如图 [8-15](#Fig15) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig15_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig15_HTML.jpg)

图 8-15

ADF 选项 3 用于提取管道活动指标的源查询

接收器将连接到之前创建的 SQL 数据库管道日志表。

在保存、发布和运行管道之后，注意管道复制活动记录已经被捕获到`dbo.pipeline_log`表中，如图 [8-16](#Fig16) 所示。

![../images/511918_1_En_8_Chapter/511918_1_En_8_Fig16_HTML.jpg](../images/511918_1_En_8_Chapter/511918_1_En_8_Fig16_HTML.jpg)

图 8-16

pipeline_log 表结果的 SSMS 视图，它将 ADF 选项 3 管道数据记录到 SQL DW 中

## 摘要

在本章中，我演示了如何使用自定义方法记录 ADF 管道数据，该方法从 ADF 管道中提取每个表或文件的指标，然后将结果写入`pipeline_parameter`表、`pipeline_log`表或 ADLS Gen2 中的 CSV 文件。此自定义日志记录流程可添加到多个 ADF 管道活动步骤中，以实现强大且可重复使用的日志记录和审计流程，该流程可在 SSMS 通过自定义 SQL 查询轻松查询，甚至可链接到 Power BI 仪表板和报告，以促进对日志记录数据的强大报告。在下一章，我将演示如何将 ADF 管道中的错误细节记录到 Azure SQL 表中。