# 5.计划扩展

> *行动表达轻重缓急。*
> 
> —圣雄甘地
> 
> 印度律师、政治家、社会活动家、作家

调度器是 Kubernetes 的核心部分，用于将工作负载分配给集群中的节点。分配操作基于集群管理员和操作员设置的优先级和规则。本章将着重于通过创建定制调度程序和开发扩展来扩展 Kubernetes 调度程序。在本章的最后，您将在集群中同时运行多个调度程序。此外，您将通过创建调度程序扩展程序来干预调度决策。

让我们从 Kubernetes 调度器及其扩展点的概述开始。

## 库调度程序概述

Kubernetes 调度程序在控制平面中运行，并将 pod 分配给节点。默认行为是平衡节点的资源利用率，同时应用集群中资源的规则和优先级。调度器的原理遵循 Kubernetes 的控制器设计模式。它会观察新创建的 pod，并在集群中找到最佳节点。

让我们通过从头创建一个多节点集群和一个 pod 来看看 Kubernetes 调度程序的运行情况。

```
$  minikube start --nodes 5

Listing 5-1Starting a multi-node local cluster

```

该命令将创建一个包含五个节点的本地集群，您可以使用以下命令列出这些节点。

```
$ kubectl get nodes
NAME           STATUS   ROLES    AGE     VERSION
minikube       Ready    master   7m17s   v1.19.2
minikube-m02   Ready    <none>   5m43s   v1.19.2
minikube-m03   Ready    <none>   4m10s   v1.19.2
minikube-m04   Ready    <none>   2m22s   v1.19.2
minikube-m05   Ready    <none>   34s     v1.19.2

Listing 5-2Node listing

```

现在，让我们创建一个 pod 并等待它运行。

```
$ kubectl run nginx-1 --image=nginx
pod/nginx-1 created
$ kubectl get pods -w
NAME      READY   STATUS               RESTARTS     AGE
nginx-1   0/1     Pending              0            0s
nginx-1   0/1     Pending              0            0s
nginx-1   0/1     ContainerCreating    0            0s
nginx-1   1/1     Running              0            16s

Listing 5-3Pod creation

```

你将在几秒钟内看到`Pending`、`ContainerCreating`和`Running`阶段。调度的关键步骤是`Pending`。它表明 Kubernetes API 接受了 pod，但是它还没有被安排到集群节点。让我们检查事件，以找到任何与调度相关的信息。

```
$ kubectl get events
...
2m54s       Normal   Scheduled                 pod/nginx-1         Successfully assigned default/nginx-1 to minikube-m05
...

Listing 5-4Event listing

```

您将看到`kube-scheduler`已经选择了`minikube-m05`节点。让我们深入了解一下`kube-scheduler`的内部结构，了解更多关于决策是如何做出的。

### 调度框架

调度框架是 Kubernetes 调度器的架构。它是一个可插入的框架，插件在其中实现调度特性。框架中的顺序工作流有多个步骤，如图 [5-1](#Fig1) 所示。工作流程主要分为*调度*和*绑定*两种。调度的重点是寻找最佳节点，而绑定处理 Kubernetes API 操作来完成调度。

![../images/503015_1_En_5_Chapter/503015_1_En_5_Fig1_HTML.jpg](../images/503015_1_En_5_Chapter/503015_1_En_5_Fig1_HTML.jpg)

图 5-1

调度框架

每个步骤都有不言自明的名称，但是有一些要点需要考虑:

*   `QueueSort`:在`kube-scheduler`的等待队列中对待调度的 pod 进行排序。

*   `PreFilter`:检查与调度周期相关的 pod 的条件和信息。

*   `Filter`:过滤节点，通过使用插件和调用外部调度程序扩展程序，为 pod 找到合适的节点列表。

*   `PostFilter`:如果没有可行节点，则运行可选步骤。在一个典型的场景中，`PostFilter`将导致其他 pod 的抢占，从而为调度打开一些空间。

*   `PreScore`:为评分插件创建一个可共享状态。

*   `Score/Prioritize`:通过调用各个评分插件和调度器扩展程序，对过滤后的节点进行排名。

*   综合多个来源的得分，计算出最终排名。具有最高加权分数的节点将赢得 pod。

*   `Reserve/Unreserve`:通知插件关于所选节点的信息是一个可选步骤。

*   `Permit`:批准、拒绝或暂停(超时)调度决策。

*   `PreBind`:在将 pod 绑定到节点之前，执行所需的任何工作，例如提供网络卷并安装它。

*   `Bind`:该步骤只由一个插件处理，因为它需要将决策发送给 Kubernetes API。

*   `PostBind`:通知装订循环结果的可选信息步骤。

一个插件可以在多个工作流点注册并执行调度子任务。虽然框架和插件创建了一个开放的架构，但是所有的插件都被编译成了`kube-scheduler`二进制文件。您可以从参考文档中查看可用插件的列表。如果你想改变控制平面中运行的`kube-scheduler`的配置，这是终极知识。

我们已经看到了调度程序的运行，并对其架构有所了解。现在我们将继续定义扩展点以及如何使用它们。

#### 扩展点

您可以用四种主要方法定制或扩展 Kubernetes 调度程序。

第一种方式是克隆修改上游`kube-scheduler`代码。然后，您需要编译、封装和运行，而不是在控制平面中部署`kube-scheduler`。然而，这并不那么简单，而且需要付出巨大的努力，在下一个版本中调整上游代码的变化。

第二种方式是为`kube-scheduler`内部的调度框架开发插件。这不是第一种方法 *hacky* ，但是同样，它需要和第一种方法一样多的努力，因为您需要更新、编译和维护上游`kube-scheduler`存储库的变更。

第三种方法是在集群中运行一个单独的调度程序和默认的调度程序。在`PodSpec`中有一个特定的字段来定义调度器:`schedulerName`。如果该字段为空，则将其设置为`default-scheduler`，并由`kube-scheduler`处理。因此，可以运行第二个调度程序，并在`schedulerName`字段中指定它。然后，自定义调度程序会将 pod 分配给节点。这种方法实现了控制器 Kubernetes 设计模式。它将*监视*具有特定`schedulerName`的 pod，并为它们分配一个节点。

第四种也是最后一种方法是开发和运行调度程序扩展程序。调度器扩展器是外部服务器，Kubernetes 调度器在调度框架的特定步骤调用它们。这种方法类似于调度框架插件，但是扩展程序是带有 HTTP 端点的外部服务。因此，扩展程序实现了 webhook Kubernetes 设计模式。

前两个扩展方法不是真正的扩展点，因为它们修改了 vanilla Kubernetes 组件。因此，在这一章中，我们将关注最后两种方式:多调度器和调度器扩展程序。我们可以在图 [5-2](#Fig2) 中说明这两种方法与 Kubernetes 调度程序的交互。

![../images/503015_1_En_5_Chapter/503015_1_En_5_Fig2_HTML.jpg](../images/503015_1_En_5_Chapter/503015_1_En_5_Fig2_HTML.jpg)

图 5-2

立方调度程序扩展点

三个阶段与调度程序扩展程序交互:`Filter`、`Prioritize`和`Bind`。因此，使用扩展器在`kube-scheduler`的规则内操作是有益的。如果您正在寻求更大的灵活性，选择运行自定义调度程序是明智的。自定义调度程序是外部应用程序，因此它们不限于调度框架的流程和请求。

在接下来的小节中，您将了解这两种方法的细节，并看到它们的实际应用。

## 配置和管理多个调度程序

Kubernetes scheduler 以其精细的架构和丰富的配置功能将 pod 分配给节点。但是，如果默认计划程序不符合您的要求，可以创建一个新的计划程序并同时运行它们。多调度器的基本思想是基于 pod 规范中的一个字段:`schedulerName`。如果指定了字段，则 pod 由相应的调度程序调度。另一方面，如果没有设置，默认调度程序将调度 pod。

让我们从运行`minikube start --nodes 5`创建一个多节点集群开始，如果您还没有启动和运行集群的话。然后，您可以创建一个 pod 并检查它是否为`schedulerName`。

```
$ kubectl run nginx-by-default-scheduler --image=nginx

$ kubectl get pods
NAME                         READY   STATUS    RESTARTS   AGE
nginx-by-default-scheduler   1/1     Running   0          99s

$ kubectl get pods nginx-by-default-scheduler -o jsonpath="{.spec.schedulerName}"
default-scheduler

Listing 5-5Pod with the default scheduler

```

当您在没有指定`schedulerName`字段的情况下创建时，它由缺省值填充，然后由缺省调度程序分配。现在，让我们创建另一个由定制调度程序处理的 pod。

```
$ kubectl run nginx-by-custom-scheduler --image=nginx --overrides='{"spec":{"schedulerName":"custom-scheduler"}}'
pod/nginx-by-custom-scheduler created

$ kubectl get pods nginx-by-custom-scheduler
NAME                        READY   STATUS    RESTARTS   AGE
nginx-by-custom-scheduler   0/1     Pending   0          16s

Listing 5-6Pod with a custom scheduler

```

pod 处于`Pending`状态，因为没有调度程序来处理它。现在是时候为集群部署第二个调度程序来处理`schedulerName`字段等于`custom-scheduler`的 pod 了。

在`custom-scheduler`中，我们将禁用上游调度程序中启用的所有 *beta* 功能。调度程序将在`kube-scheduler`旁边的`kube-system`名称空间中运行。创建一个名为`kube-scheduler-custom.yaml`的文件，内容如下。

```
apiVersion: v1
kind: Pod
metadata:
  name: kube-scheduler-custom
  namespace: kube-system
spec:
  containers:
  - name: kube-scheduler-custom
    image: k8s.gcr.io/kube-scheduler:v1.19.0
    command:
    - kube-scheduler
    - --kubeconfig=/etc/kubernetes/scheduler.conf
    - --leader-elect=false
    - --scheduler-name=custom-scheduler
    - --feature-gates=AllBeta=false
    volumeMounts:
    - mountPath: /etc/kubernetes/scheduler.conf
      name: kubeconfig
      readOnly: true
  nodeName: minikube
  restartPolicy: Always
  volumes:
  - hostPath:
      path: /etc/kubernetes/scheduler.conf
      type: FileOrCreate
    name: kubeconfig

Listing 5-7Custom scheduler pod definition

```

pod 是运行`k8s.gcr.io/kube-scheduler:v1.19.0`映像并将`kubeconfig`附加为只读卷的坦率定义。以下三个标志定义了自定义计划程序的功能:

*   `leader-elect=false`在运行调度程序之前禁用领导者选举阶段，因为只有一个自定义调度程序实例会运行。

*   `scheduler-name=custom-scheduler`定义调度程序的名称。

*   `feature-gates=AllBeta=false`禁用所有测试版功能。

使用`kubectl apply -f kube-scheduler-custom.yaml`文件创建部署并检查 pod 状态。

```
$  kubectl -n kube-system get pods kube-scheduler-custom
NAME                    READY   STATUS    RESTARTS   AGE
kube-scheduler-custom   1/1     Running   0          24s

Listing 5-8Custom scheduler pod in the cluster

```

现在，检查我们的 pod 的状态，它卡在 Pending 中。

```
$ kubectl get pods nginx-by-custom-scheduler
NAME                        READY   STATUS    RESTARTS   AGE
nginx-by-custom-scheduler   1/1     Running   0          42s

Listing 5-9Pod assignment

```

pod 处于`Running`阶段，这意味着定制调度程序可以完美地工作。创建定制调度器可能不是每个云工程师日常工作的一部分，因为默认的 Kubernetes 调度器在大多数情况下都工作得很好。然而，当您需要实现更复杂的需求时，您将创建您的定制调度程序并将其部署到集群中。让我们假设您想要创建一个调度程序来最小化成本。在您的自定义调度程序中，您可能需要首先将 pod 分配给最便宜的节点。相反，您可以创建一个自定义调度程序，在选择节点时考虑监视指标。在这种情况下，您可能需要将 pod 分发到节点，以最小化系统中的总延迟。然而，最小化成本或优化延迟取决于外部系统，而不在默认的 Kubernetes 调度程序的范围内。

运行多个调度器并用它们想要的调度器标记 pod 是一种简单的 Kubernetes-native 方法。最重要的部分是开发一个防弹调度程序。创建和操作自定义调度程序时，有三个关键点需要考虑:

*   **Kubernetes API 兼容性** : Scheduler 与 Kubernetes API 交互，以观察 pod、检索节点列表并创建绑定。因此，您需要开发与 Kubernetes API 版本兼容的定制调度程序。如果您使用的是官方客户端库，幸运的是，您只需要使用正确的版本。

*   **高可用性**:如果您的调度程序停止运行或出现故障，将导致 pod 处于挂起状态。因此，您的应用程序将不会在集群中运行。因此，您需要将应用程序设计为高可用性运行。

*   **配合默认调度器**:如果集群中有不止一个决策者，您需要小心冲突的决策。例如，默认调度程序控制资源请求和限制。如果您的自定义计划程序在不考虑群集资源的情况下填充节点，默认计划程序的窗格可能会移动到其他节点。因此，您的定制调度程序应该与默认调度程序配合良好，并避免决策冲突。

在下面的练习中，您将使用`kubebuilder`从头开始创建一个定制调度程序。此外，您将在集群中运行它，并将一些 pod 分配给节点。

EXERCISE: DEVELOPING A CUSTOM SCHEDULER WITH KUBEBUILDER

在本练习中，您将使用`kubebuilder`创建一个定制的*混沌*调度器。本质上，调度程序是监视集群中的 pod 的控制器。因此，您将创建一个控制器并实现协调方法。最后，您将运行控制器并看到它的运行。

**注意**剩下的练习是基于`kubebuilder`的，它需要以下先决条件:`kubebuilder` v2.3.1，Go 版本 v1.14+，访问一个 Kubernetes 集群，以及`kubectl`。

*   检索节点列表。

*   随机选择一个节点。

*   创建一个包含节点和 pod 的绑定资源。

*   将绑定资源发送给 Kubernetes API。

1.  Initialize the project structure with the following commands:

    ```
    $ mkdir -p $GOPATH/src/extend-k8s.io/chaos-scheduler
    $ cd $GOPATH/src/extend-k8s.io/chaos-scheduler
    $ kubebuilder init
    Writing scaffold for you to edit...
    Get controller runtime:
    $ go get sigs.k8s.io/controller-runtime@v0.5.0
    Update go.mod:
    $ go mod tidy
    Running make:
    $ make
    .../bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..."
    go fmt ./...
    go vet ./...
    go build -o bin/manager main.go
    Next: define a resource with:
    $ kubebuilder create api

    ```

    这些命令创建一个文件夹，并用样板代码引导项目。

2.  Create controller for watching the pods with the following command:

    ```
    $ kubebuilder create api --kind Pod --group core --version v1
    Create Resource [y/n]
    n
    Create Controller [y/n]
    y
    Writing scaffold for you to edit...
    controllers/pod_controller.go
    Running make:
    $ make
    .../bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..."
    go fmt ./...
    go vet ./...
    go build -o bin/manager main.go

    ```

    由于 pods 已经是 Kubernetes 资源，选择`no`跳过`Create Resource`提示。但是，接受第二个提示，为 pod 资源生成一个控制器。

3.  打开位于`controllers`文件夹的`pod_controlller.go`。你会看到两个功能`SetupWithManager`和`Reconcile`。`SetupWithManager`是控制器启动时调用的函数。`Reconcile`是由集群中每个观察到的变化调用的函数。

    Change the `SetupWithManager` function with the following content:

    ```
    func (r *PodReconciler) SetupWithManager(mgr ctrl.Manager) error {

          filter := predicate.Funcs{
                CreateFunc: func(e event.CreateEvent) bool {
                      pod, ok := e.Object.(*corev1.Pod)
                      if ok {
                            if pod.Spec.SchedulerName == "chaos-scheduler" && pod.Spec.NodeName == "" {
                                  return true
                            }
                            return false
                      }
                      return false
                },
                UpdateFunc: func(e event.UpdateEvent) bool {
                      return false
                },
                DeleteFunc: func(e event.DeleteEvent) bool {
                      return false
                },
          }

          return ctrl.NewControllerManagedBy(mgr).
                For(&corev1.Pod{}).
                WithEventFilter(filter).
                Complete(r)
    }

    ```

    它添加了一个过滤器来观察带有`schedulerName chaos-scheduler`和空`nodeName`的 pod 的创建事件。

    Change the `Reconcile` function with the following content:

    ```
    func (r *PodReconciler) Reconcile(req ctrl.Request) (ctrl.Result, error) {
          ctx := context.Background()
          log := r.Log.WithValues("pod", req.NamespacedName)

          nodes := new(corev1.NodeList)
          err := r.Client.List(ctx, nodes)
          if err != nil {
                return ctrl.Result{Requeue: true}, err
          }

          node := nodes.Items[rand.Intn(len(nodes.Items))].Name
          log.Info("scheduling", "node", node)

          binding := new(corev1.Binding)
          binding.Name = req.Name
          binding.Namespace = req.Namespace
          binding.Target = corev1.ObjectReference{
                Kind:       "Node",
                APIVersion: "v1",
                Name:       node,
          }

          err = r.Client.Create(ctx, binding)
          if err != nil {
                return ctrl.Result{Requeue: true}, err
          }

          return ctrl.Result{}, nil
    }

    ```

    更新后的`Reconcile`功能执行以下操作:

选择一个随机节点是调度器制造混乱的基本部分。它将测试 Kubernetes 在动荡和意外条件下的能力和恢复力。

**注**混沌工程是一种在不断变化的情况下(即混沌)对系统进行实验的常用方法。该方法对大规模和分布式应用进行试验，以建立对弹性和复原力的信心。

将以下库添加到`pod_controlller.go`的导入列表中:

1.  Start the controller with the following command:

    ```
    $ make run
    ../bin/controller-gen object:headerFile="hack/boilerplate.go.txt" paths="./..."
    go fmt ./...
    go vet ./...
    ../bin/controller-gen "crd:trivialVersions=true" rbac:roleName=manager-role webhook paths="./..." output:crd:artifacts:config=config/crd/bases
    go run ./main.go
    INFO      controller-runtime.metrics    metrics server is starting to listen    {"addr": ":8080"}
    INFO      setup          starting manager
    INFO      controller-runtime.manager    starting metrics server           {"path": "/metrics"}
    INFO      controller-runtime.controller    Starting EventSource      {"controller": "pod", "source": "kind source: /, Kind="}
    INFO      controller-runtime.controller    Starting Controller      {"controller": "pod"}
    INFO      controller-runtime.controller    Starting workers   {"controller": "pod", "worker count": 1}

    ```

    如日志所示，控制器启动并等待集群中 pod 的事件。

2.  在另一个终端，创建一个由`chaos-scheduler` :

    ```
    $ kubectl run nginx-by-chaos-scheduler --image=nginx --overrides='{"spec":{"schedulerName":"chaos-scheduler"}}'
    pod/nginx-by-chaos-scheduler created

    ```

    安排的 pod
3.  Check the logs of controller started in Step 4:

    ```
    ...
    INFO      controllers.Pod      scheduling      {"pod": "default/nginx-by-chaos-scheduler", "node": "minikube-m02"}
    DEBUG      controller-runtime.controller    Successfully Reconciled    {"controller": "pod", "request": "default/nginx-by-chaos-scheduler"}

    ```

    额外的日志行表示自定义计划程序分配了 pod。

4.  检查在步骤 5 中启动的 pod 的状态:

    ```
    $ kubectl get pods nginx-by-chaos-scheduler
    NAME                      READY  STATUS    RESTARTS  AGE
    nginx-by-chaos-scheduler  1/1    Running   0         34s

    ```

```
"math/rand"
"sigs.k8s.io/controller-runtime/pkg/event"
"sigs.k8s.io/controller-runtime/pkg/predicate"

```

混沌调度程序将新的 pod 分配给一个节点，它正在运行。它显示了使用`kubebuilder`从头开始开发的定制调度程序可以完美地工作。

在下一节中，我们将使用第二个扩展点来扩展 Kubernetes 调度程序:调度程序扩展程序。scheduler extender 方法将作为 webhooks 工作，并干扰调度框架阶段。

## 调度程序扩展器

调度器扩展器是外部的 webhooks，用于在调度框架的不同阶段调整调度决策。框架有多个阶段寻找合适的节点，每一步都调用编译成`kube-scheduler`的插件。在四个特定的阶段，它还调用调度器扩展程序:*过滤器*、*评分/优先化*、*抢占*，以及*绑定*。来自 webhooks 的响应与调度器插件的结果相结合。因此，调度程序扩展程序方便了对`kube-scheduler`的扩展，而无需深入其源代码。

在本节中，将介绍配置细节和扩展器 API。最后，您将开发一个 scheduler extender webhook 服务器，并在 Kubernetes 集群中运行它。

### 配置详细信息

Kubernetes 调度程序连接到外部进程，因此它应该知道在哪里连接和评估响应。配置通过一个模式为`KubeSchedulerConfiguration`的文件传递。最低配置如下所示。

```
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /etc/kubernetes/scheduler.conf

Listing 5-10Minimal KubeSchedulerConfiguration

```

Note

`KubeSchedulerConfiguration`的全面细节可在[参考文件中获得。](https://kubernetes.io/docs/reference/scheduling/config/)

您也可以按如下方式向`KubeSchedulerConfiguration`添加扩展器。

```
apiVersion: kubescheduler.config.k8s.io/v1beta1
kind: KubeSchedulerConfiguration
clientConnection:
  kubeconfig: /etc/kubernetes/scheduler.conf
extenders:
- urlPrefix: http://localhost:8888/
  filterVerb: filter
  ignorable: true
  weight: 1
- urlPrefix: http://localhost:8890/
  filterVerb: filter
  prioritizeVerb: prioritize
  bindVerb: bind
  ignorable: false
  weight: 1

Listing 5-11Extenders in KubeSchedulerConfiguration

```

在前面的例子中，两个扩展器在`localhost:8888`和`localhost:8890`上运行。第一个仅用于过滤节点，当它失败时，它不会阻止调度。然而，第二个是在框架的过滤、评分和绑定阶段调用的。此外，它是不可忽略的，所以如果 webhook 不可达或失败，pod 的调度将被卡在`Pending`。

Note

您可以在[源代码](https://github.com/kubernetes/kubernetes/blob/master/staging/src/k8s.io/kube-scheduler/config/v1beta1/types.go)中检查扩展器配置的字段，因为它不是 Kubernetes API 文档的一部分。

在用扩展器信息配置了`kube-scheduler`之后，现在让我们深入研究它们之间的交互。

#### 调度程序扩展器 API

Kubernetes scheduler 使用与其阶段相关的数据对扩展程序进行 HTTP 调用，并期望得到结构化的响应。在 scheduler extender 中，您需要用 JSON 请求和响应来实现这些调用。最重要的优点是，您可以用独立于 Kubernetes 二进制文件的任何语言来开发扩展器。

##### 过滤器

Filter webhooks 接收以下数据作为参数。

```
type ExtenderArgs struct {
      // Pod being scheduled
      Pod *v1.Pod
      // List of candidate nodes where the pod can be
      // scheduled; to be populated only if
      // Extender.NodeCacheCapable == false
      Nodes *v1.NodeList
      // List of candidate node names where the pod
      // can be scheduled; to be populated only if
      // Extender.NodeCacheCapable == true
      NodeNames *[]string
}

Listing 5-12ExtenderArgs data structure

```

它只是由一个 pod 和一个基于扩展器中缓存状态的节点或节点名称列表组成。作为响应，下面的数据结构被发送回来。

```
type ExtenderFilterResult struct {
      // Filtered set of nodes where the pod can be scheduled
      // only if Extender.NodeCacheCapable == false
      Nodes *v1.NodeList
      // Filtered set of nodes where the pod can be scheduled
      // only if Extender.NodeCacheCapable == true
      NodeNames *[]string
      // Filtered out nodes where the pod can't be scheduled
      // and the failure messages
      FailedNodes FailedNodesMap
      // Error message indicating failure
      Error string
}

type FailedNodesMap map[string]string

Listing 5-13ExtenderFilterResult data structure

```

响应由 pod 的过滤节点组成。此外，不可调度的节点作为`FailedNodes`与它们的消息一起被发回。最后，如果过滤由于任何原因失败，还有一个`Error`字段。

##### 优先考虑

优先化 webhooks 接收相同的数据结构`ExtenderArgs`，就像过滤器 webhooks 一样。webhook 应该为节点创建分数，以分配 pod 并发回以下数据结构。

```
type HostPriorityList []HostPriority

type HostPriority struct {
      // Name of the host
      Host string
      // Score associated with the host
      Score int64
}

Listing 5-14HostPriorityList data structure

```

来自 webhook 的分数被添加到由其他扩展器和 Kubernetes scheduler 插件计算的分数中。调度框架为 pod 分配选择具有最高分数的节点。

##### 先取

当 Kubernetes 将 pod 调度到节点时，在集群中找到合适的节点并不总是可能的。在这种情况下，抢占逻辑被触发以从节点中驱逐一些 pod。如果抢占成功，pod 将被调度到节点，被驱逐的将找到新家。在抢占过程中，调度器还使用以下数据结构调用启用的 webhooks。

```
type ExtenderPreemptionArgs struct {
      //pod being scheduled
      Pod *v1.Pod
      // Victims map generated by scheduler preemption phase
      // Only set NodeNameToMetaVictims if
      // Extender.NodeCacheCapable == true.
      // Otherwise, only set NodeNameToVictims.
      NodeNameToVictims     map[string]*Victims
      NodeNameToMetaVictims map[string]*MetaVictims
}

type Victims struct {
       // a group of pods expected to be preempted.
      Pods             []*v1.Pod
      // the count of violations of PodDisruptionBudget
      NumPDBViolations int64
}

type MetaVictims struct {
       // a group of pods expected to be preempted.
      Pods             []*v1.Pod
      // the count of violations of PodDisruptionBudget
      NumPDBViolations int64
}

Listing 5-15ExtenderPreemptionArgs data structure

```

数据由一个 pod 和一个潜在节点图组成，这些节点上有`Victims`。作为响应，webhook 发送以下数据。

```
type ExtenderPreemptionResult struct {
      NodeNameToMetaVictims map[string]*MetaVictims
}

Listing 5-16ExtenderPreemptionResult data structure

```

webhook 评估抢占的节点和单元，并发送回潜在的受害者。

##### 约束

绑定调用用于委托节点和 pod 分配。当它被实现时，与 Kubernetes API 交互以进行绑定就成了扩展器的责任。Webhooks 接收以下数据作为参数。

```
type ExtenderBindingArgs struct {
      // PodName is the name of the pod being bound
      PodName string
      // PodNamespace is the namespace of the pod being bound
      PodNamespace string
      // PodUID is the UID of the pod being bound
      PodUID types.UID
      // Node selected by the scheduler
      Node string
}

Listing 5-17ExtenderBindingArgs data structure

```

作为响应，如果在绑定期间发生错误，它将返回。

```
type ExtenderBindingResult struct {
      // Error message indicating failure
      Error string
}

Listing 5-18ExtenderBindingResult data structure

```

在下面的练习中，您将从头开始创建一个 scheduler extender，并实际使用它。扩展器将干扰调度框架决策和 pod 到节点的分配。

EXERCISE: DEVELOPING AND RUNNING A SCHEDULER EXTENDER

在本练习中，您将创建一个定制的 chaos scheduler 扩展器，并在 Kubernetes 集群中运行它。您将在 Go 中开发一个 HTTP web 服务器，因为调度程序扩展程序原则上是 webhook 服务器。此外，您将在 minikube 中配置`kube-scheduler`以连接到您的调度程序扩展器。

**注意**剩下的练习是基于在 Go 中编写一个 web 服务器，它需要以下先决条件:Docker、minikube 和`kubectl`。

1.  使用以下命令在`minikube`中启动多节点集群:

1.  Create a pod definition for scheduler extender with the name `kube-scheduler-extender.yaml` under `manifests` folder with the following content:

    ```
    apiVersion: v1
    kind: Pod
    metadata:
      labels:
        component: kube-scheduler-extender
        tier: control-plane
      name: kube-scheduler-extender
      namespace: kube-system
    spec:
      containers:
      - image: DOCKER_REPOSITORY/k8s-scheduler-extender:v1
        name: kube-scheduler-extender
      hostNetwork: true

    ```

    **注意**不要忘记将`DOCKER_REPOSITORY`更改为步骤 8 中设置的环境变量。

2.  使用以下命令将当前工作目录挂载到`minikube`节点:

    ![../images/503015_1_En_5_Chapter/503015_1_En_5_Figa_HTML.png](../images/503015_1_En_5_Chapter/503015_1_En_5_Figa_HTML.png)

3.  In another terminal, SSH into the `minikube` node and copy the manifests with the following commands, and restart the `kubelet`:

    ```
    $ minikube ssh
    docker@minikube:~$ sudo su

    root@minikube:/home/docker# cp /etc/k8s-scheduler-extender/manifests/kube-scheduler-extender.yaml /etc/kubernetes/manifests/kube-scheduler-extender.yaml

    root@minikube:/home/docker# cp /etc/k8s-scheduler-extender/manifests/kube-scheduler-config.yaml /etc/kubernetes/kube-scheduler-config.yaml

    root@minikube:/home/docker# cp /etc/k8s-scheduler-extender/manifests/kube-scheduler.yaml /etc/kubernetes/manifests/kube-scheduler.yaml

    root@minikube:/home/docker# systemctl restart kubelet

    ```

    在复制步骤中，您已经将清单和配置文件添加到了`kubelet`查找的位置。在最后一步中，您已经重启了`kubelet`来加载新文件并使用它们。您可以退出 minikube 节点并在本地工作站上继续。

4.  创建一个有 25 个副本的部署，并观察集群中的事件:

    ```
    $ kubectl create deployment nginx --image=nginx --replicas=25
    deployment.apps/nginx created
    $  kubectl get events --field-selector reason=FailedScheduling
    LAST SEEN   TYPE      REASON             OBJECT                       MESSAGE
    ...
    3m23s       Warning   FailedScheduling   pod/nginx-6799fc88d8-qnn9p   0/5 nodes are available: 1 nginx-6799fc88d8-qnn9p cannot be scheduled to minikube-m02: coin is tails, 1 nginx-6799fc88d8-qnn9p cannot be scheduled to minikube-m03: coin is tails, 1 nginx-6799fc88d8-qnn9p cannot be scheduled to minikube-m04: coin is tails, 1 nginx-6799fc88d8-qnn9p cannot be scheduled to minikube-m05: coin is tails, 1 nginx-6799fc88d8-qnn9p cannot be scheduled to minikube: coin is tails.
    ...

    ```

    如果您运气好，所有五个节点都有 tails，那么 pod 也会有类似的事件。如果您没有幸运地看到该事件，您还可以检查调度程序扩展器的日志:

    ```
    $ kubectl -n kube-system logs -f kube-scheduler-extender-minikube
    ...
    time=".." level=info msg="Flipped the coin and it is heads"
    time=".." level=info msg="Flipped the coin and it is heads"
    time=".." level=info msg="Flipped the coin and it is tails"
    time=".." level=info msg="Flipped the coin and it is heads"
    time=".." level=info msg="Flipped the coin and it is tails"
    time=".." level=info msg="Rolled the dice and it is 1"
    time=".." level=info msg="Rolled the dice and it is 10"
    time=".." level=info msg="Rolled the dice and it is 1"
    ...

    ```

*   命令标志`config`

*   名为`kube-scheduler-config`的卷

*   卷`kube-scheduler-config`的卷安装

1.  Create the following folder structure in your Go environment:

    ```
    $ mkdir -p cd $GOPATH/src/extend-k8s.io/k8s-scheduler-extender
    $ cd $GOPATH/src/extend-k8s.io/k8s-scheduler-extender
    $ mkdir -p cmd manifests pkg/filter pkg/prioritize
    $ tree -a

    .
    ├── cmd
    ├── manifests
    └── pkg
        ├── filter
        └── prioritize

    5 directories, 0 files

    ```

    文件夹结构是创建 Go 应用程序的主流方式。在下面的步骤中，您将在每个目录中创建文件。

2.  Create a file `flip.go` in `pkg/filter` folder with the following content:

    ```
    package filter

    import (
          "math/rand"
          "time"

          "github.com/sirupsen/logrus"
    )

    const (
          HEADS = "heads"
          TAILS = "tails"
    )

    var coin []string

    func init() {
          rand.Seed(time.Now().UnixNano())
          coin = []string{HEADS, TAILS}
    }

    func Flip() string {

          side := coin[rand.Intn(len(coin))]
          logrus.Info("Flipped the coin and it is ", side)
          return side
    }

    ```

    函数`Flip`返回正面或反面来随机过滤节点。

    Create a file `filter.go` in `pkg/filter` folder with the following content:

    ```
    package filter

    import (
          "fmt"

          corev1 "k8s.io/api/core/v1"
          extenderv1 "k8s.io/kube-scheduler/extender/v1"
    )

    func Filter(args extenderv1.ExtenderArgs) extenderv1.ExtenderFilterResult {

          filtered := make([]corev1.Node, 0)
          failed := make(extenderv1.FailedNodesMap)

          pod := args.Pod

          for _, node := range args.Nodes.Items {

                side := Flip()
                if side == HEADS {
                      filtered = append(filtered, node)
                } else {
                      failed[node.Name] = fmt.Sprintf("%s cannot be scheduled to %s: coin is %s", pod.Name, node.Name, side)
                }
          }

          return extenderv1.ExtenderFilterResult{
                Nodes: &corev1.NodeList{
                      Items: filtered,
                },
                FailedNodes: failed,
          }

    }

    ```

    `Filter`函数通过接收`ExtenderArgs`作为参数和`ExtenderFilterResult`作为响应来实现调度程序扩展器调用的逻辑。

3.  Create a file `roll.go` in `pkg/prioritize` folder with the following content:

    ```
    package prioritize

    import (
          "math/rand"
          "time"

          "github.com/sirupsen/logrus"
          extenderv1 "k8s.io/kube-scheduler/extender/v1"
    )

    func init() {
          rand.Seed(time.Now().UnixNano())
    }

    func Roll() int64 {

          number := rand.Int63n(extenderv1.MaxExtenderPriority + 1)
          logrus.Info("Rolled the dice and it is ", number)

          return number

    }

    ```

    `Roll` function imitates rolling dice to find a score for the nodes. Create a file `prioritize.go` in `pkg/prioritize` folder with the following content:

    ```
    package prioritize

    import (
          extenderv1 "k8s.io/kube-scheduler/extender/v1"
    )

    func Prioritize(args extenderv1.ExtenderArgs) extenderv1.HostPriorityList {

          hostPriority := make(extenderv1.HostPriorityList, 0)

          for _, node := range args.Nodes.Items {
                hostPriority = append(hostPriority, extenderv1.HostPriority{
                      Host:  node.Name,
                      Score: Roll(),
                })
          }

          return hostPriority

    }

    ```

    `Prioritize`函数实现调度器扩展器调用来接收`ExtenderArgs`并将`HostPriorityList`发送回`kube-scheduler`。

4.  Create a `main.go` file under `cmd` folder with the following content:

    ```
    package main

    import (
          "encoding/json"
          "log"
          "net/http"

          "github.com/gorilla/mux"
          "github.com/extend-k8s.io/k8s-scheduler-extender/pkg/filter"
          "github.com/extend-k8s.io/k8s-scheduler-extender/pkg/prioritize"
          "github.com/sirupsen/logrus"

          extenderv1 "k8s.io/kube-scheduler/extender/v1"
    )

    func main() {
          r := mux.NewRouter()

          r.HandleFunc("/", homeHandler)
          r.HandleFunc("/filter", filterHandler)
          r.HandleFunc("/prioritize", prioritizeHandler)

          log.Fatal(http.ListenAndServe(":8888", r))
    }

    func filterHandler(w http.ResponseWriter, r *http.Request) {

          args := extenderv1.ExtenderArgs{}
          response := extenderv1.ExtenderFilterResult{}

          if err := json.NewDecoder(r.Body).Decode(&args); err != nil {
                response.Error = err.Error()
          } else {
                response = filter.Filter(args)
          }

          w.Header().Set("Content-Type", "application/json")
          if err := json.NewEncoder(w).Encode(response); err != nil {
                logrus.Error(err)
                return
          }

    }

    func prioritizeHandler(w http.ResponseWriter, r *http.Request) {

          args := extenderv1.ExtenderArgs{}
          response := make(extenderv1.HostPriorityList, 0)

          if err := json.NewDecoder(r.Body).Decode(&args); err == nil {
                response = prioritize.Prioritize(args)
          }

          w.Header().Set("Content-Type", "application/json")
          if err := json.NewEncoder(w).Encode(response); err != nil {
                logrus.Error(err)
                return
          }
    }

    func homeHandler(w http.ResponseWriter, r *http.Request) {

          w.Write([]byte("scheduler extender is running!"))
    }

    ```

    它是带有 HTTP 处理程序的 webhook 服务器的入口点，用于过滤和区分调用的优先级。默认情况下，服务器将在 8888 端口上运行。

5.  在根文件夹中创建一个`go.mod`文件来设置依赖版本:

    ```
    module github.com/extend-k8s.io/k8s-scheduler-extender

    go 1.14

    require (
          github.com/gorilla/mux v1.8.0
          github.com/sirupsen/logrus v1.6.0
          k8s.io/api v0.19.0
          k8s.io/kube-scheduler v0.19.0
    )

    ```

6.  在根文件夹中创建一个`Dockerfile`来构建容器镜像，步骤如下:

    ```
    FROM golang:1.14-alpine as builder
    ADD . /go/src/github.com/extend-k8s.io/k8s-scheduler-extender
    WORKDIR /go/src/github.com/extend-k8s.io/k8s-scheduler-extender/cmd
    RUN go build -v

    FROM alpine:latest
    COPY --from=builder /go/src/github.com/extend-k8s.io/k8s-scheduler-extender/cmd/cmd /usr/local/bin/k8s-scheduler-extender
    CMD ["k8s-scheduler-extender"]

    ```

7.  现在，您可以使用以下命令构建并推送调度程序扩展器的 Docker 映像:

    **Note** Set `DOCKER_REPOSITORY` environment variable according to your Docker repository.

    ```
    $ docker build -t $DOCKER_REPOSITORY/k8s-scheduler-extender:v1 .
    Step 1/7 : FROM golang:1.14-alpine as builder
    ...
    Step 7/7 : CMD ["k8s-scheduler-extender"]
     ---> Running in 6655404206c1
    Removing intermediate container 6655404206c1
     ---> 0ce4bb201541
    Successfully built 0ce4bb201541
    Successfully tagged $DOCKER_REPOSITORY/k8s-scheduler-extender:v1

    $ docker push $DOCKER_REPOSITORY/k8s-scheduler-extender:v1
    The push refers to repository [docker.io/$DOCKER_REPOSITORY/k8s-scheduler-extender]
    ...
    v1: digest: sha256:0e62a24a4b9e9e0215f5f02e37b5f86d9235ee950e740069f80951e370ae5b34 size: 739

    ```

8.  Create a Kubernetes scheduler configuration file with the name `kube-scheduler-config.yaml` under `manifests` folder:

    ```
    apiVersion: kubescheduler.config.k8s.io/v1beta1
    kind: KubeSchedulerConfiguration
    clientConnection:
      kubeconfig: /etc/kubernetes/scheduler.conf
    extenders:
    - urlPrefix: http://localhost:8888/
      filterVerb: filter
      prioritizeVerb: prioritize
      weight: 1

    ```

    这是一个将被传递给`kube-scheduler`的简单配置，它用端点定义了您的扩展器的位置。

9.  Create a Kubernetes scheduler pod file to replace the default pod definition of `kube-scheduler`. Set the filename `kube-scheduler.yaml` under `manifests` folder with the following content:

    ```
    apiVersion: v1
    kind: Pod
    metadata:
      creationTimestamp: null
      labels:
        component: kube-scheduler
        tier: control-plane
      name: kube-scheduler
      namespace: kube-system
    spec:
      containers:
      - command:
        - kube-scheduler
        - --authentication-kubeconfig=/etc/kubernetes/scheduler.conf
        - --authorization-kubeconfig=/etc/kubernetes/scheduler.conf
        - --bind-address=127.0.0.1
        - --kubeconfig=/etc/kubernetes/scheduler.conf
        - --leader-elect=false
        - --port=0
        - --config=/etc/kubernetes/kube-scheduler-config.yaml
        image: k8s.gcr.io/kube-scheduler:v1.19.0
        imagePullPolicy: IfNotPresent
        livenessProbe:
          failureThreshold: 8
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 10259
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 15
        name: kube-scheduler
        resources:
          requests:
            cpu: 100m
        startupProbe:
          failureThreshold: 24
          httpGet:
            host: 127.0.0.1
            path: /healthz
            port: 10259
            scheme: HTTPS
          initialDelaySeconds: 10
          periodSeconds: 10
          timeoutSeconds: 15
        volumeMounts:
        - mountPath: /etc/kubernetes/scheduler.conf
          name: kubeconfig
          readOnly: true
        - mountPath: /etc/kubernetes/kube-scheduler-config.yaml
          name: kube-scheduler-config
          readOnly: true
      hostNetwork: true
      priorityClassName: system-node-critical
      volumes:
      - hostPath:
          path: /etc/kubernetes/scheduler.conf
          type: FileOrCreate
        name: kubeconfig
      - hostPath:
          path: /etc/kubernetes/kube-scheduler-config.yaml
          type: FileOrCreate
        name: kube-scheduler-config
    status: {}

    ```

    它添加了三个部分来使用步骤 9 中的`kube-scheduler-config.yaml`。

```
$ minikube start --kubernetes-version v1.19.0 --nodes 5

```

这表明`kube-scheduler`配置正确，并且连接到调度程序扩展器 webhook。webhook 通过投掷硬币来随机过滤节点。此外，它通过掷骰子给节点打分。换句话说，scheduler extender 在调度过程中产生了一些随机性和混乱。

开发和运行调度程序扩展程序非常简单，因为您可以扩展现有的默认调度程序的功能，而无需重新编译二进制文件。此外，您可以用任何想要的编程语言创建扩展程序。但是，最好在以下问题上保持谨慎，因为您会生成 Kubernetes 控制平面组件的接触点:

*   **配置**:使用静态文件为 Kubernetes 调度程序定义扩展程序。因此，请确保文件位置及其内容是正确的。此外，确保该文件不会因集群升级而被覆盖或删除。

*   **性能**:像所有的 webhooks 一样，扩展器作为外部进程运行。连接到另一个服务器并检索响应在时间上是很昂贵的。确保 webhook 尽可能快地提供响应，并且控制平面组件可以访问它。

*   **缓存不一致**:可以在扩展器中为节点信息启用缓存。如果您的节点不经常改变，或者调度决策不那么重要，那么将节点信息缓存在扩展器中是有益的。另一方面，如果您总是需要关于节点的最新信息，可以禁用缓存，使用 Kubernetes 调度程序发送给您的数据。

## 关键要点

*   Kubernetes 调度器是在集群上分配工作负载的控制平面组件。

*   Kubernetes 调度程序在优先级和规则集中选择最佳节点。

*   通过在集群中运行多个调度器，可以扩展调度决策。

*   调度框架是 Kubernetes 调度器的可插拔架构，并且可以通过 webhooks 进行扩展。

在下一章中，我们将通过开发和运行存储、网络和设备插件来扩展 Kubernetes 与基础设施的交互。